{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e14b24f0a798c076077d1b46ec0327d3fdb5214e4c8fe7e4032fe90708d84898"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de mathématiques (UE11.1)\n",
    "HERNOT Anatole<br>LEBOEUF Antoine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   THIRD-PARTY LIBRARIES\n",
    "from typing import Callable\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffFunctions():\n",
    "\n",
    "    def grad(self, f:Callable):\n",
    "        g = autograd.grad\n",
    "        def grad_f(x, y):\n",
    "            x, y = float(x), float(y)\n",
    "            return np.array([g(f, 0)(x, y), g(f, 1)(x, y)])\n",
    "        return grad_f\n",
    "\n",
    "    def J(self, f:Callable):\n",
    "        j = autograd.jacobian\n",
    "        def J_f(x, y):\n",
    "            x, y = float(x), float(y)\n",
    "            return np.array([j(f, 0)(x, y), j(f, 1)(x, y)]).T\n",
    "        return J_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Question 1</b>\n",
    "\n",
    "Soit $c \\in \\mathbb{R}$.\n",
    "On suppose que la fonction $f:\\mathbb{R}^2 \\to \\mathbb{R}$ est continue et vérifie\n",
    "$f(x_1, x_2) \\to +\\infty$ quand $\\|(x_1,x_2)\\| \\to +\\infty$.\n",
    "Que peut-on dire de l'ensemble de niveau $c$ de $f$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $c \\in \\mathbb{R}$. On définit l'ensemble de niveau $c$ :<br>\n",
    "$$\\mathrm{LVL}(c) = \\left\\{ (x_1, x_2) \\mid f(x_1,x_2) = c \\right\\}$$\n",
    "* Montrons que l'ensemble de niveau $c$ de $f$ est <b>borné</b>.<br>\n",
    "$f(x_1,x_2) \\xrightarrow[\\|(x_1,x_2)\\| \\to +\\infty]{} +\\infty$. Alors $\\exists M(c) > 0, \\ \\|(x_1,x_2)\\| \\geq M(c) \\ \\Rightarrow \\ f(x_1,x_2) > c$. Donc $\\mathrm{LVL}(c) \\subset \\left\\{(x_1, x_2) \\mid \\|(x_1,x_2)\\| < M(c) \\right\\} = E_M(c)$, un ensemble borné.\n",
    "\n",
    "* Montrons que l'ensemble de niveau $c$ de $f$ est <b>fermé</b>.<br>\n",
    "morphisme continu?\n",
    "\n",
    "\n",
    "On travaille dans $\\mathbb{R}^2$ de dimension finie, donc $\\mathrm{LVL}(c)$, étant fermé et borné dans $\\mathbb{R}^2$, est un <b>compact</b> de $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, la fonction $f$ est supposée continûment différentiable. On suppose également que le gradient $\\nabla f$ ne s'annule pas dans un voisinage du point $x_0 = (x_{10}, x_{20}) \\in \\mathbb{R}^2$. On pose alors\n",
    "$$\n",
    "p(x_1, x_2) := \\frac{\\partial_2 f(x_0)}{\\|\\nabla f(x_0)\\|} (x_1 - x_{10}) -\n",
    "\\frac{\\partial_1 f(x_0)}{\\|\\nabla f(x_0)\\|} (x_2 - x_{20}).\n",
    "$$\n",
    "\n",
    "## <b>Question 2</b>\n",
    "Comment interpréter géométriquement le terme $p(x_1,x_2)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_3 = p(x_1,x_2)$ est l'équation d'un plan (un hyperplan de $\\mathbb{R}^3$). En changeant légèrement les notations :\n",
    "\n",
    "$$\n",
    "p(x, y)\n",
    "\\ =\\ \n",
    "\\frac{1}{\\| \\nabla f(X_0) \\|} \\left( \\frac{\\partial f}{\\partial y}(X_0) (x - x_0) - \\frac{\\partial f}{\\partial x}(X_0) (y - y_0) \\right)\n",
    "\\ =\\ \n",
    "\\frac{1}{\\| \\nabla f(X_0) \\|}\n",
    "\\begin{vmatrix}\n",
    "\\partial_y f (x_0, y_0) & y-y_0\n",
    "\\\\\n",
    "\\partial_x f (x_0, y_0) & x-x_0\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Notons $K_\\mathrm{grad} = \\frac{1}{\\| \\nabla f(X_0) \\|}$. $p(x_0, y_0) = 0$.\n",
    "\n",
    "$p_{y_0} : x \\mapsto p(x, y_0) = \\partial_y f (x_0, y_0) K_\\mathrm{grad} \\cdot (x-x_0)$ est une fonction affine.\n",
    "\n",
    "$p_{x_0} : y \\mapsto p(x_0, y) = - \\partial_x f (x_0, y_0) K_\\mathrm{grad} \\cdot (y-y_0)$ est une fonction affine.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$x_3 = \\| \\nabla f(x_0) \\| \\times p(x_1, x_2)$ est l'équation d'un plan orthgonal au plan $x_3 = f(x_1, x_2)$ en $x_0$.\n",
    "\n",
    "<br><br><br>\n",
    "INTERPRÉTATION GÉOMÉTRIQUE???????????\n",
    "C'est peut-être la projection orthogonale sur la droite\n",
    "$$\n",
    "d : y = \\frac{\\partial_2 f(x_0, y_0)}{\\partial_1 f(x_0, y_0)} (x - x_0) + y_0\n",
    "$$\n",
    "mais c'est un plan, dans $\\mathbb{R}^3$ !! :(\n",
    "\n",
    "\n",
    "\n",
    "Si $x_0, y_0 = 0, 0$ (translation de l'origine) :\n",
    "$p(x, y) = Ax + By$\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "<b>Nouvelle idée :</b><br><br>\n",
    "\n",
    "Dans un souci de simplification, on notera en majuscule le doublet de coordonnées de $\\mathbb{R}^2$ correspondant à un point $i$ dans le plan $(O, x, y)$ : $X_i = (x_i, y_i)$. Travaillant au plus dans $\\mathbb{R}^3$ de dimension $3$, on identifiera les coordonnées $x_1$, $x_2$, et $x_3$, utilisées précédemment, à $x$, $y$, et $z$.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "On se place dans un voisinage de $X_0 = (x_0, y_0)$ : alors on peut approximer les dérivées de $f$ par ses différentielles, au premier ordre (approximation affine autour de $X_0$) (???).\n",
    "<br>Donc $\\| \\nabla f (x_0, y_0) \\| \\times p(x_1, y_1) = \\big( f(x_1, y_0) - \\underbrace{f(x_0, y_0)}_{= c} \\big) - \\big( f(x_0, y_1) - \\underbrace{f(x_0, y_0)}_{= c} \\big) = f(x_1, y_0) - f(x_0, y_1)$.<br>Ainsi, en première approximation, pour $X_1 = (x_1, y_1)$ choisi dans un 'bon voisinage' de $X_0$, $p(x_1, y_1) \\simeq f(x_1, y_0) - f(x_0, y_1)$.\n",
    "\n",
    "En réalité, en notant $z = \\tilde{f}(x, y)$ le plan tangent à $f$ en $(x_0, y_0)$, on obtient $p(x_1, y_1) = \\tilde{f}(x_1, y_0) - \\tilde{f}(x_0, y_1)$. $\\tilde{f}$ est l'approximation affine (linéaire) de $f$ dans un voisinage de $X_0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Cela fournit une aide précieuse pour l'interprétation géométrique de $p(x, y)$, qui apparaît alors comme (???)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Montrer que dans un voisinage ouvert de $x_0$, on peut paramétriser l'ensemble de niveau $c:=f(x_0)$ au moyen de $p(x_1,x_2),$ c'est-à-dire qu'il existe un $\\varepsilon > 0$ et une fonction (continûment différentiable) $\\gamma :\\left]-\\varepsilon,\\varepsilon \\right[ \\to \\mathbb{R}^2$ tels que dans un voisinage ouvert de $x_0,$ $f(x_1,x_2) = c$ si et seulement si $(x_1, x_2) = \\gamma(t)$ où $t = p(x_1, x_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Montrer que pour tout $t \\in \\left]-\\varepsilon, \\varepsilon \\right[$ :\n",
    "\n",
    "  - le vecteur $\\gamma'(t)$ est non nul (il fournit donc une tangente au chemin $\\gamma$),\n",
    "\n",
    "  - est orthogonal à $\\nabla f(\\gamma(t))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "L'application à laquelle nous destinons la fonction `Newton` demande-t'elle une grande précision ?\n",
    "Choisir une valeur de `eps` qui semble raisonnable et justifier l'ordre de grandeur choisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   GLOBAL VARIABLES\n",
    "eps = 10**-6\n",
    "N = 10**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFunc = DiffFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   FUNCTIONS\n",
    "def Newton(F:Callable, x0:float, y0:float, eps:float=eps, N:int=N, debug:bool=False) -> tuple:\n",
    "\n",
    "    \"\"\"\n",
    "    This function solves the equation F(x,y) = 0 around (x0,y0) using the Newton algorithm.\n",
    "\n",
    "    :param F: The function to solve for\n",
    "    :param x0: The initial x-axis coordinate\n",
    "    :param y0: The initial y-axis coordinate\n",
    "    :param eps: The acceptable precision of the algorithm\n",
    "    :param N: The maximum number of iterations (will raise an error if exceeded)\n",
    "\n",
    "    :returns: The solution to the equation F(x,y) = 0, to a precision of eps\n",
    "    \"\"\"\n",
    "\n",
    "    #   0. Troubleshooting types (ugly)\n",
    "    x0, y0 = float(x0), float(y0)\n",
    "\n",
    "    #   1. Defining an iteration counter\n",
    "    iter_counter = 0\n",
    "\n",
    "    #   2. Running the method in a loop to refine the calculation\n",
    "    while True:\n",
    "        \n",
    "        #   2.1. Generating the gradient of F (n-dimensional derivative)\n",
    "        gradF = DFunc.grad(F)\n",
    "\n",
    "        #   2.2.1. Getting the value of the gradient of F at point (x0,y0)\n",
    "        gradF_appl1 = gradF(x0,y0)\n",
    "        #   2.2.2. Calculating new x-coordinate for a better approximation of the solution | x_k+1 = x_k - [ d_x( f(x_k, y) ) ]^-1 / f(x_k, y)\n",
    "        x = x0 - F(x0, y0) / gradF_appl1[0]\n",
    "        #   2.2.3. Getting the value of the gradient of F at point(x,y0)\n",
    "        gradF_appl2 = gradF(x,y0)\n",
    "        #   2.2.4. Calculating the new y-coordinate for a better approximation of the solution | x_k+1 = x_k - [ d_x( f(x_k, y) ) ]^-1 / f(x_k, y)\n",
    "        y = y0 - F(x, y0) / gradF_appl2[1]\n",
    "\n",
    "        #   2.3. Breaking the loop and returning the solution when the precision condition (with eps) is met\n",
    "        if np.sqrt((x - x0)**2 + (y - y0)**2) <= eps:\n",
    "            if debug: print(f'Solution found for (x,y) = {(x, y)}, with value F(x,y) = {F(x, y)}\\nIterations required for calculation: {iter_counter} / {N} ({round(iter_counter/N*100, 1)}%)') # DEBUG\n",
    "            return x, y\n",
    "        \n",
    "        #   2.4. Setting the values for the next iteration\n",
    "        x0, y0 = x, y\n",
    "\n",
    "        #   2.5. Incrementing the iteration counter\n",
    "        iter_counter += 1\n",
    "\n",
    "        #   2.6. Breaking the loop when the max number of iterations is reached\n",
    "        if iter_counter >= N:\n",
    "            break\n",
    "    \n",
    "\n",
    "    #   3. Raising an error when no solution is found and the max number of iterations is exceeded\n",
    "    raise ValueError(f'No convergence in {N} steps.')"
   ]
  }
 ]
}